LARGE_MODEL = "claude-3-opus-20240229" # The large model to use

# SMALL_MODEL = "claude-3-haiku-20240307" # The small model to use
SMALL_MODEL = "glm-4"

import openai
import re

openai.api_key = "sk-xpA4UDDjmVQU8oRp0cA2EaC9111f401483A2E9E957C02dBb"
openai.base_url = "http://127.0.0.1:3000/v1/"

def generate_candidate_prompts(task, prompt_example, response_example):
    messages = [{
            "role": "system",
            "content":"""<task>Given an example training sample, create seven additional samples for the same task that are even better. Each example should contain a <prompt> and a <response>.</task>

<rules>
1. Ensure the new examples are diverse and unique from one another.
2. They should all be perfect. If you make a mistake, this system won't work.
</rules>

Respond in this format:
<response_format>
<example_one>
<prompt>
PUT_PROMPT_HERE
</prompt>
<response>
PUT_RESPONSE_HERE
</response>
</example_one>

<example_two>
<prompt>
PUT_PROMPT_HERE
</prompt>
<response>
PUT_RESPONSE_HERE
</response>
</example_two>

...
</response_format>"""
        }, {
            "role": "user",
            "content": f"""<training_task>{task}</training_task>

<prompt_example>
{prompt_example}
</prompt_example>

<response_example>
{response_example}
</response_example>"""},
    ]

    response = openai.chat.completions.create(
        model=LARGE_MODEL,
        max_tokens=4000,
        temperature=0.5,
        messages=messages,
    )
    response_text = response.choices[0].message.content

    # Parse out the prompts and responses
    prompts_and_responses = []
    examples = re.findall(r'<example_\w+>(.*?)</example_\w+>', response_text, re.DOTALL)
    for example in examples:
        prompt = re.findall(r'<prompt>(.*?)</prompt>', example, re.DOTALL)[0].strip()
        response = re.findall(r'<response>(.*?)</response>', example, re.DOTALL)[0].strip()
        prompts_and_responses.append({'prompt': prompt, 'response': response})

    return prompts_and_responses

def generate_system_prompt(task, prompt_examples):
    messages = [
        {"role": "system", "content": """<your_role>Given a user-description of their <task> a set of prompt / response pairs (it'll be in JSON for easy reading) for the types of outputs we want to generate given inputs, write a fantastic system prompt that describes the task to be done perfectly.</your_role>

<rules>
1. Do this perfectly.
2. Respond only with the system prompt, and nothing else. No other text will be allowed.
</rules>

Respond in this format:
<system_prompt>
WRITE_SYSTEM_PROMPT_HERE
</system_prompt>"""
        },
        {"role": "user", "content": f"""<task>{task}</task>

<prompt_response_examples>
{str(prompt_examples)}
</prompt_response_examples>"""
        }]

    response = openai.chat.completions.create(
        model=LARGE_MODEL,
        max_tokens=1000,
        temperature=0.5,
        messages=messages,
    )
    response_text = response.choices[0].message.content

    # Parse out the prompt
    system_prompt = response_text.split('<system_prompt>')[1].split('</system_prompt>')[0].strip()

    return system_prompt

def test_haiku(generated_examples, prompt_example, system_prompt):
    messages = [{"role": "system", "content": system_prompt}]

    for example in generated_examples:
      messages.append({"role": "user", "content": example['prompt']})
      messages.append({"role": "assistant", "content": example['response']})

    messages.append({"role": "user", "content": prompt_example.strip()})

    response = openai.chat.completions.create(
        model=SMALL_MODEL,
        max_tokens=2000,
        temperature=0.5,
        messages=messages,
    )
    response_text = response.choices[0].message.content

    return response_text

def run_haiku_conversion_process(task, prompt_example, response_example):

    print('Generating the prompts / responses...')
    # Generate candidate prompts
    generated_examples = generate_candidate_prompts(task, prompt_example, response_example)

    print('Prompts / responses generated. Now generating system prompt...')

    # Generate the system prompt
    system_prompt = generate_system_prompt(task, generated_examples)

    print('System prompt generated:', system_prompt)


    print('\n\nTesting the new prompt on '+SMALL_MODEL+', using your input example...')
    # Test the generated examples and system prompt with the Haiku model
    small_model_response = test_haiku(generated_examples, prompt_example, system_prompt)

    print(SMALL_MODEL+' responded with:')
    print(small_model_response)

    print('\n\n!! CHECK THE FILE DIRECTORY, THE PROMPT IS NOW SAVED THERE !!')

    # Create a dictionary with all the relevant information
    result = {
        "task": task,
        "initial_prompt_example": prompt_example,
        "initial_response_example": response_example,
        "generated_examples": generated_examples,
        "system_prompt": system_prompt,
        "small_model_response": small_model_response
    }

    # Save the Haiku prompt to a Python file
    with open("haiku_prompt.py", "w") as file:
        file.write('system_prompt = """' + system_prompt + '"""\n\n')

        file.write('messages = [\n')
        for example in generated_examples:
            file.write('    {"role": "user", "content": """' + example['prompt'] + '"""},\n')
            file.write('    {"role": "assistant", "content": """' + example['response'] + '"""},\n')

        file.write('    {"role": "user", "content": """' + prompt_example.strip() + '"""}\n')
        file.write(']\n')

    return result



# task = "refactoring complex code"
task = "æ ¹æ®æä¾›ä¿¡æ¯å†™ä¸€ä»½æ—…æ¸¸æŒ‡å—"

prompt_example = """è¿™ä»½é è°±çš„è¥¿å®‰æ¸¸ç©æ”»ç•¥ï¼Œä½ èƒ½æƒ³è±¡æ˜¯ä¸€ä½ç†å·¥ç§‘çš„ç”·å‹å†™çš„å—ğŸ¤£çœŸçš„å¤ªä¸å¯æ€è®®äº†ï¼Œå“ˆå“ˆå“ˆå“ˆï¼ï¼ï¼å„ä½å§å¦¹ä»¬å¯ä»¥è®¤çœŸçœ‹å®Œå“¦ğŸ¥³
-
ğŸ’¦è¥¿å®‰è¡Œç¨‹å®‰æ’
ã€ğŸ“ç¬¬â‘ å¤©ã€‘é’Ÿæ¥¼ï¼é¼“æ¥¼ï¼è¥¿å®‰åŸå¢™ï¼å›æ°‘è¡—ï¼é«˜å®¶å¤§é™¢ï¼ˆçœ‹çš®å½±æˆå“¦ï¼‰
ã€ğŸ“ç¬¬â‘¡å¤©ã€‘é™•è¥¿å†å²åšç‰©é¦†ï¼ˆå‘¨ä¸€é—­é¦†ï¼Œéœ€é¢„çº¦ï¼‰ï¼æ´’é‡‘æ¡¥ï¼å¤§é›å¡”ï¼ˆå¯ä»¥ä¸ç”¨ç™»å¡”ï¼‰ï¼å¤§å”ä¸å¤œåŸï¼å¤§å”èŠ™è“‰å›­
ã€ğŸ“ç¬¬â‘¢å¤©ã€‘åæ¸…æ± ï¼éªŠå±±ï¼ç§¦å§‹çš‡å…µé©¬ä¿‘ï¼ˆå»ºè®®è¯·è®²è§£ï¼Œæ—©ç‚¹å»ï¼‰ï¼ä¸½å±±å›­ï¼é•¿æ¨æ­Œï¼ˆå¾ˆéœ‡æ’¼çš„è¡¨æ¼”ï¼‰"""
# prompt_example = """def calculate_total(prices, tax, discount, shipping_fee, gift_wrap_fee, membership_discount):

#     total = 0

#     for i in range(len(prices)):

#         total += prices[i]

#     if membership_discount != 0:

#         total = total - (total * (membership_discount / 100))

#     if discount != 0:

#         total = total - (total * (discount / 100))

#     total = total + (total * (tax / 100))

#     if total < 50:

#         total += shipping_fee

#     else:

#         total += shipping_fee / 2

#     if gift_wrap_fee != 0:

#         total += gift_wrap_fee * len(prices)

#     if total > 1000:

#         total -= 50

#     elif total > 500:

#         total -= 25

#     total = round(total, 2)

#     if total < 0:

#         total = 0

#     return total"""

response_example = """è¥¿å®‰ï¼Œä½œä¸ºâ¼—ä¸‰æœå¤éƒ½
æˆ‘å‰å‰ååå»äº†5æ¬¡
çœŸçš„æ˜¯å¥½åƒï¼Œå¥½æ‹ï¼Œå¥½é€›çš„åŸå¸‚â¤ï¸
è¿™æ¬¡åˆåœ¨è¥¿å®‰æš´èµ°3å¤©
-
æ•´ç†äº†1âƒ£ï¸8âƒ£ï¸ä¸ªè¶…å‡ºç‰‡çš„æœºä½ï¼Œè·Ÿç€èµ°å°±å¯¹äº†â€¼ï¸
-
1âƒ£ï¸å¤§æ…ˆæ©å¯ºé—å€å…¬å›­
å…è´¹å…¬å›­ï¼Œå¯ä»¥getä½›åƒå’Œå¤§é›å¡”åŒæ¡†
2âƒ£ï¸è¥¿å®‰åŸå¢™ï¼ˆæ°¸å®é—¨ï¼‰
å¯¼èˆªâ€œæ°¸å®é—¨â€ï¼Œè¥¿ä¾§åŸå¢™ï¼Œç™½å¤©&å¤œæ™¯éƒ½å¥½çœ‹
3âƒ£ï¸å¼€å…ƒå•†åœºäº”æ¥¼ï¼ˆé¼“æ¥¼åº—ï¼‰
è¿™é‡Œå¯ä»¥ä¿¯ç°é¼“æ¥¼ï¼Œå¤œæ™¯ç»ç¾
4âƒ£ï¸æ°¸å®é—¨
å”®ç¥¨å¤„ä¸Šæ¥¼æ­£å¯¹ç€å¯ä»¥æ‹åˆ°é¼“æ¥¼
5âƒ£ï¸å¹¿ä»å¯º
æ²¿ç€å·¦ä¾§çš„çº¢å¢™èµ°å°±æ˜¯å‡ºç‰‡æœºä½
6âƒ£ï¸é’é¾™å¯º
ç›¸å¯¹æ¯”è¾ƒå†·é—¨çš„åœ°æ–¹ï¼Œä½†æ˜¯å¾ˆæœ‰å¤§å”ç››ä¸–çš„éŸµå‘³ã€‚
7âƒ£ï¸å¤§æ‚¦åŸé¡¶æ¥¼
æ‹å¤§é›å¡”çš„å¥½æœºä½ï¼Œæ™šä¸Šå¤§é›å¡”ä¼šäº®ç¯
8âƒ£ï¸å­ä½©é›†
åœ¨å›æ°‘è¡—ï¼Œæ—©ä¸Š9ç‚¹å¼€é—¨å°±å»ï¼Œä¸ç„¶äººä¼šå¤š
9âƒ£ï¸æ¹˜å­åº™è¡—
æœ‰å„ç§å¥½çœ‹çš„åº—é“ºï¼Œè¿™å®¶çœŸçš„å¾ˆæ˜¾çœ¼å‡ºç‰‡ã€‚
1âƒ£ï¸0âƒ£ï¸é¼“æ¥¼
ç«™åœ¨å—ä¾§è·¯å¯¹é¢å¯ä»¥æ‹åˆ°å®Œæ•´çš„å¤æ¥¼
1âƒ£ï¸1âƒ£ï¸å¤§é›å¡”å—å¹¿åœº
é¡ºæ—¶é’ˆèµ°ä¸€åœˆï¼Œæœ‰ä¸å°‘è·Ÿå¤§é›å¡”åŒæ¡†çš„æœºä½
1âƒ£ï¸2âƒ£ï¸è´°å›å··ç«é”…é¦†ï¼ˆé’Ÿæ¥¼åº—ï¼‰
é—¨å£æœ‰â€œé•¿å®‰â€å­—æ ‡
1âƒ£ï¸3âƒ£ï¸è¾£æ–—è¾£ç«é”…ï¼ˆé’Ÿæ¥¼åº—ï¼‰
é—¨å£æœ‰â€œè¥¿å®‰â€å­—æ ‡
1âƒ£ï¸4âƒ£ï¸é™†ç¦»æ±‰æœ
é—¨å£æœ‰â€œé•¿å®‰â€å­—æ ‡
1âƒ£ï¸5âƒ£ï¸å¤§é›å¡”å—å¹¿åœºï¼ˆå—é—¨å£ï¼‰
1âƒ£ï¸6âƒ£ï¸æ°¸å®é—¨ï¼ˆé è¿‘å‡ºå£çš„ä½ç½®ï¼‰
1âƒ£ï¸7âƒ£ï¸æ°¸å®é—¨ï¼ˆé è¿‘å‡ºå£çš„ä½ç½®ï¼‰
1âƒ£ï¸8âƒ£ï¸æ°¸å®é—¨ï¼ˆè¥¿å®‰åŸå¢™å¤œæ™¯ï¼‰
-
ğŸš¶ã€Œæˆ‘çš„3å¤©2æ™šæ¸¸ç©è·¯çº¿ã€
Day1:å¤§é›å¡”â€”å”å¤§æ…ˆæ©å¯ºâ€”å¤§æ‚¦åŸâ€”é’Ÿæ¥¼
Day2:å¹¿ä»å¯ºâ€”ä¹¦é™¢é—¨â€”æ°¸å®é—¨åŸå¢™
Day3:å›æ°‘è¡—â€”é’é¾™å¯ºâ€”æ¹˜å­åº™è¡—
-
ğŸœã€Œç¾é£Ÿæ”»ç•¥ã€
èƒ¡è¾£æ±¤ï¼Œç¾Šè‚‰æ³¡é¦è¿™ç§é’ˆå¯¹æ¸¸å®¢çš„è¯•è¯•å°±è¡Œï¼Œæœ¬åœ°äººå¸¸å»çš„æ´’é‡‘æ¡¥å’Œæ¹˜å­åº™è¡—ï¼Œæ´’é‡‘æ¡¥å°åƒå¤šï¼Œæ¥åœ°æ°”ï¼Œæ¹˜å­åº™è¡—å’–å•¡åº—å¤šæ›´æ—¶é«¦ç°ä»£ï¼Œé€‚åˆæ‰“å¡ã€‚
-
è·Ÿç€è¿™ä»½è¯¦ç»†æ”»ç•¥èµ°ï¼Œä¸å‡ºé”™ã€‚å¦‚æœä¸æƒ³äººå¤šçš„ï¼Œä¹Ÿå¯ä»¥é¿å¼€èŠ‚å‡æ—¥ï¼Œé”™å³°å‡ºè¡Œï¼Œæ‹ç…§ä½“éªŒè·Ÿæ›´å¥½ã€‚"""
# response_example = """def calculate_total(prices, tax_rate, discount_rate, shipping_fee, gift_wrap_fee, membership_discount_rate):

#     def apply_percentage_discount(amount, percentage):

#         return amount * (1 - percentage / 100)

#     def calculate_shipping_fee(total):

#         return shipping_fee if total < 50 else shipping_fee / 2

#     def apply_tier_discount(total):

#         if total > 1000:

#             return total - 50

#         elif total > 500:

#             return total - 25

#         return total

#     subtotal = sum(prices)

#     subtotal = apply_percentage_discount(subtotal, membership_discount_rate)

#     subtotal = apply_percentage_discount(subtotal, discount_rate)



#     total = subtotal * (1 + tax_rate / 100)

#     total += calculate_shipping_fee(total)

#     total += gift_wrap_fee * len(prices)



#     total = apply_tier_discount(total)

#     total = max(0, round(total, 2))



#     return total"""

result = run_haiku_conversion_process(task, prompt_example, response_example)